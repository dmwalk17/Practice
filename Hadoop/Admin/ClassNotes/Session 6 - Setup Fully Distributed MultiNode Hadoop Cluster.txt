============================================================
BigData Analytics & Hadoop - Training - Session 6
============================================================

Trainer Name - Prashant Shantigrama
Email ID - prashanth.grama@hotmail.com

Hadoop Web UI
=============

NameNode ----------------- 50070
JobTracker --------------- 50030
TaskTracker -------------- 50060
SecondaryNameNode -------- 50090


Setup Hadoop in Fully Distributed Mode (Creating a 5 Node Hadoop Cluster)
=========================================================================

1. Create 5 new Virtual machines by cloning the psuedo-distributed setup we created in previous session

	(Ensure that you power-off the machine that you are cloning from)
	- Right Click on the JuneHadoopBatch machine in the VMWare workstation
	- Right Click on "Manage" option and choose "Clone" option
	- Under the cloning method....choose "Full Clone" option
	- Give a name to the VM (NameNode Server, CheckpointServer, SlaveNode1, SlaveNode2, SlaveNode3)

NameNodeServer ------------------ 192.168.11.170
CheckpointServer ---------------- 192.168.11.131
SlaveNode1 ---------------------- 192.168.11.132
SlaveNode2 ---------------------- 192.168.11.133

2. Configure the NameNode and JobTracker services for the cluster (On all the Nodes of the cluster)

	Ensure that all 4 nodes contain same core-site.xml and mapred-site.xml file contents which should point to the NameNodeServer

	sudo vi /usr/local/hadoop/conf/core-site.xml
	sudo vi /usr/local/hadoop/conf/mapred-site.xml

	Change the ipaddress of the NameNode and JobTracker services to the ip of NameNodeServer

3. Delete and Recreate hdfs folders and assign the permission for hadoop user (On all the Nodes of the cluster)

	sudo rm -r /usr/local/hadoop/hdfs
	sudo mkdir /usr/local/hadoop/hdfs
	sudo chown hadoop /usr/local/hadoop/hdfs

4. Configure the SecondaryNameNode service for the cluster (Only on NameNodeServer)

	sudo vi /usr/local/hadoop/conf/masters

	Add/change the ipaddress to the checkpoint server

5. Configure DataNode and TaskTracker services for the cluster (Only on NameNodeServer)

	sudo vi /usr/local/hadoop/conf/slaves

	Add the ipaddress of SlaveNode1 and SlaveNode2 

6. Create a password-less setup for the cluster ( Only on NameNodeServer)

	ssh-keygen

	cat /home/hadoop/.ssh/id_rsa.pub > /home/hadoop/.ssh/authorized_keys

	>>> We need to transfer the keys from NameNodeServer to all other Nodes
	ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@(respective ip addresses)

	ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@192.168.11.131
	ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@192.168.11.132
	ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@192.168.11.133


7.  Format the NameNode (Only on NameNode Server)

	hadoop namenode -format

8. Start all the Hadoop services (Only on the NameNodeServer)

	start-all.sh


Adding a new node into the cluster
==================================

	SlaveNode3 ----------- 192.168.11.138

1. Ensure the core-site.xml and mapred-site.xml point to NameNodeServer of the cluster (ONLY on the New Node)
	
	sudo vi /usr/local/hadoop/conf/core-site.xml
	sudo vi /usr/local/hadoop/conf/mapred-site.xml

	Change the ipaddress of the NameNode and JobTracker services to the ip of NameNodeServer

2. Delete and Recreate hdfs folder and assign permission for hadoop user ( ONLY on the New Node)

	sudo rm -r /usr/local/hadoop/hdfs
	sudo mkdir /usr/local/hadoop/hdfs
	sudo chown hadoop /usr/local/hadoop/hdfs	

3. Inform the NameNode about the new SlaveNode (ONLY on the NameNodeServer)

	sudo vi /usr/local/hadoop/conf/slaves

	Add the ipaddress of SlaveNode3

4. Transfer the public rsa tokens into the new Node (ONLY on the NameNodeServer)

	ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@192.168.11.138

5. Start the DataNode and TaskTracker services on the New Node (ONLY on the New Node)

	hadoop-daemon.sh start datanode
	hadoop-daemon.sh start tasktracker


6. Rebalance the cluster ( ONLY on the NameNodeServer)

	start-balancer.sh


Decommissioning a Node from the Cluster
=======================================

1. Create a file in the NameNodeServer called excludes (ONLY on NameNodeServer)

	sudo vi /home/hadoop/excludes

	Add the ipaddresses of the nodes that you want to decommission

2. Inform HDFS about the excludes list (ONLY on NameNode Server)

	sudo vi /usr/local/hadoop/conf/hdfs-site.xml

	dfs.hosts.exclude ----> filename of the excludes list

	<property>
	<name>dfs.hosts.exclude</name>
	<value>/home/hadoop/excludes</value>
	</property>

3. Refresh the Nodes on the cluster (Only on NameNodeServer)

	hadoop dfsadmin -refreshNodes

Tools to manage Hadoop clusters
===============================

1. Apache Ambari ------------ HortonWorks
2. Apache Ganglia
3. Cloudera Manager --------- Cloudera



